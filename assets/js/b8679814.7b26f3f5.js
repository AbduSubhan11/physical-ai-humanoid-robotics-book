"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[440],{7508:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"The AI-Robot Brain (NVIDIA Isaac)","href":"/physical-ai-humanoid-robotics-book/docs/module3_ai_robot_brain","docId":"module3_ai_robot_brain","unlisted":false},{"type":"link","label":"Module 4 \u2014 Vision-Language-Action","href":"/physical-ai-humanoid-robotics-book/docs/module4_vision_language_action","docId":"module4_vision_language_action","unlisted":false},{"type":"link","label":"Final Capstone Project - Integrating Physical AI & Humanoid Robotics","href":"/physical-ai-humanoid-robotics-book/docs/final-capstone","docId":"final-capstone","unlisted":false},{"type":"link","label":"Hardware Requirements","href":"/physical-ai-humanoid-robotics-book/docs/hardware-requirements","docId":"hardware_requirements","unlisted":false},{"type":"link","label":"Module 1 \u2014 The Robotic Nervous System (ROS 2)","href":"/physical-ai-humanoid-robotics-book/docs/module1_ros2","docId":"module1_ros2","unlisted":false},{"type":"link","label":"Module 2 \u2014 The Digital Twin (Gazebo & Unity)","href":"/physical-ai-humanoid-robotics-book/docs/module2_digital_twin","docId":"module2_digital_twin","unlisted":false},{"type":"link","label":"rclpy Example Controllers","href":"/physical-ai-humanoid-robotics-book/docs/rclpy-example-controllers","docId":"rclpy-example-controllers","unlisted":false},{"type":"link","label":"ROS 2 Nodes","href":"/physical-ai-humanoid-robotics-book/docs/ros2-nodes","docId":"ros2-nodes","unlisted":false},{"type":"link","label":"Topics & Services","href":"/physical-ai-humanoid-robotics-book/docs/topics-services","docId":"topics-services","unlisted":false},{"type":"link","label":"URDF for Humanoids","href":"/physical-ai-humanoid-robotics-book/docs/urdf-for-humanoids","docId":"urdf-for-humanoids","unlisted":false}]},"docs":{"final-capstone":{"id":"final-capstone","title":"Final Capstone Project - Integrating Physical AI & Humanoid Robotics","description":"Overview","sidebar":"tutorialSidebar"},"hardware_requirements":{"id":"hardware_requirements","title":"Hardware Requirements","description":"Essential hardware components for Physical AI & Humanoid Robotics projects.","sidebar":"tutorialSidebar"},"module1_ros2":{"id":"module1_ros2","title":"Module 1 \u2014 The Robotic Nervous System (ROS 2)","description":"The Robotic Operating System (ROS) is a flexible framework for writing robot software. It\'s a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behaviors across a wide variety of robotic platforms. ROS 2 is the latest iteration, designed with improved real-time capabilities, security, and multi-robot system support.","sidebar":"tutorialSidebar"},"module2_digital_twin":{"id":"module2_digital_twin","title":"Module 2 \u2014 The Digital Twin (Gazebo & Unity)","description":"Digital twins are virtual models designed to accurately reflect a physical object. In robotics, a digital twin allows for the simulation and testing of robots and their environments in a virtual space before deployment in the real world. This module explores two prominent platforms for creating digital twins in robotics: Gazebo and Unity.","sidebar":"tutorialSidebar"},"module3_ai_robot_brain":{"id":"module3_ai_robot_brain","title":"The AI-Robot Brain (NVIDIA Isaac)","description":"This module delves into the powerful NVIDIA Isaac platform, a comprehensive toolkit for robotics simulation, development, and deployment. We will explore its key components, including Isaac Sim for high-fidelity simulation, Isaac ROS for accelerating robotic applications, VSLAM for robust localization, Nav2 for advanced navigation, and techniques for generating photorealistic synthetic data.","sidebar":"tutorialSidebar"},"module4_vision_language_action":{"id":"module4_vision_language_action","title":"Module 4 \u2014 Vision-Language-Action","description":"This module explores the intersection of computer vision, natural language processing, and robotics. These domains converge to enable intuitive and capable robotic systems, allowing robots to understand human commands, interpret their environment, and execute complex actions.","sidebar":"tutorialSidebar"},"rclpy-example-controllers":{"id":"rclpy-example-controllers","title":"rclpy Example Controllers","description":"Implementing robotic control using rclpy.","sidebar":"tutorialSidebar"},"ros2-nodes":{"id":"ros2-nodes","title":"ROS 2 Nodes","description":"Understanding the fundamental building blocks of ROS 2.","sidebar":"tutorialSidebar"},"topics-services":{"id":"topics-services","title":"Topics & Services","description":"Exploring communication mechanisms in ROS 2.","sidebar":"tutorialSidebar"},"urdf-for-humanoids":{"id":"urdf-for-humanoids","title":"URDF for Humanoids","description":"Defining robotic structures with URDF.","sidebar":"tutorialSidebar"}}}}')}}]);