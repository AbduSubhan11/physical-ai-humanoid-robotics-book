"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[435],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var o=i(6540);const t={},a=o.createContext(t);function r(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(a.Provider,{value:n},e.children)}},8991:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module4_vision_language_action","title":"Module 4 \u2014 Vision-Language-Action","description":"This module explores the intersection of computer vision, natural language processing, and robotics. These domains converge to enable intuitive and capable robotic systems, allowing robots to understand human commands, interpret their environment, and execute complex actions.","source":"@site/docs/module4_vision_language_action.md","sourceDirName":".","slug":"/module4_vision_language_action","permalink":"/physical-ai-humanoid-robotics-book/docs/module4_vision_language_action","draft":false,"unlisted":false,"editUrl":"https://github.com/abdusubhan11/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module4_vision_language_action.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Module 4 \u2014 Vision-Language-Action"},"sidebar":"tutorialSidebar","previous":{"title":"The AI-Robot Brain (NVIDIA Isaac)","permalink":"/physical-ai-humanoid-robotics-book/docs/module3_ai_robot_brain"},"next":{"title":"Final Capstone Project - Integrating Physical AI & Humanoid Robotics","permalink":"/physical-ai-humanoid-robotics-book/docs/final-capstone"}}');var t=i(4848),a=i(8453);const r={sidebar_position:4,title:"Module 4 \u2014 Vision-Language-Action"},s="Module 4 \u2014 Vision-Language-Action",c={},d=[{value:"Whisper \u2192 Voice Commands",id:"whisper--voice-commands",level:2},{value:"Understanding Whisper",id:"understanding-whisper",level:3},{value:"Integrating Whisper for Voice Command Recognition",id:"integrating-whisper-for-voice-command-recognition",level:3},{value:"Example: Python Integration with Whisper (Safe Code Block)",id:"example-python-integration-with-whisper-safe-code-block",level:3}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4--vision-language-action",children:"Module 4 \u2014 Vision-Language-Action"})}),"\n",(0,t.jsx)(n.p,{children:"This module explores the intersection of computer vision, natural language processing, and robotics. These domains converge to enable intuitive and capable robotic systems, allowing robots to understand human commands, interpret their environment, and execute complex actions."}),"\n",(0,t.jsx)(n.h2,{id:"whisper--voice-commands",children:"Whisper \u2192 Voice Commands"}),"\n",(0,t.jsx)(n.p,{children:"Voice commands provide a natural interface for human-robot interaction. Automatic Speech Recognition (ASR) enables robots to understand spoken instructions. Whisper is a multilingual, general-purpose speech recognition model that excels even with noise or accents."}),"\n",(0,t.jsx)(n.h3,{id:"understanding-whisper",children:"Understanding Whisper"}),"\n",(0,t.jsx)(n.p,{children:"Whisper is trained on large-scale audio\u2013text data and can transcribe or translate speech. Its robustness makes it ideal for robotics, where audio may not be perfect."}),"\n",(0,t.jsx)(n.h3,{id:"integrating-whisper-for-voice-command-recognition",children:"Integrating Whisper for Voice Command Recognition"}),"\n",(0,t.jsx)(n.p,{children:"General workflow:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Capture:"})," Microphone records the user\u2019s voice."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech-to-Text Conversion:"})," Whisper transcribes the audio."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding:"})," The transcription is processed to extract intent."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Execution:"})," The robot maps intent \u2192 actions (e.g., via ROS 2)."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-python-integration-with-whisper-safe-code-block",children:"Example: Python Integration with Whisper (Safe Code Block)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\nimport pyaudio\nimport wave\n\nmodel = whisper.load_model("base")\n\ndef record_audio(filename="command.wav", duration=5):\n    CHUNK = 1024\n    FORMAT = pyaudio.paInt16\n    CHANNELS = 1\n    RATE = 16000\n\n    p = pyaudio.PyAudio()\n    stream = p.open(\n        format=FORMAT,\n        channels=CHANNELS,\n        rate=RATE,\n        input=True,\n        frames_per_buffer=CHUNK\n    )\n\n    print(f"* Recording for {duration} seconds...")\n    frames = []\n\n    for _ in range(0, int(RATE / CHUNK * duration)):\n        data = stream.read(CHUNK)\n        frames.append(data)\n\n    print("* Recording finished.")\n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n    wf = wave.open(filename, "wb")\n    wf.setnchannels(CHANNELS)\n    wf.setsampwidth(p.get_sample_size(FORMAT))\n    wf.setframerate(RATE)\n    wf.writeframes(b"".join(frames))\n    wf.close()\n\n    return filename\n\ndef transcribe_command(audio_file):\n    result = model.transcribe(audio_file)\n    return result["text"]\n\nif __name__ == "__main__":\n    audio_file = record_audio(duration=3)\n    command_text = transcribe_command(audio_file)\n    print(f"Transcribed Command: \\"{command_text}\\"")\n'})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);